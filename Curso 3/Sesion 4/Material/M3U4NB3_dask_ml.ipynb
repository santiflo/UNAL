{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92b679d6",
      "metadata": {
        "id": "92b679d6"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1YjAWn06OMcVhlyixBZBDnY17rnn7Otg5\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3859ea5e",
      "metadata": {
        "id": "3859ea5e"
      },
      "source": [
        "# Machine Learning con Dask\n",
        "\n",
        "En este notebook veremos una introducción práctica al procesamiento distribuido con la librería `dask`, primero lo instalaremos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ebdb2e5",
      "metadata": {
        "id": "0ebdb2e5"
      },
      "outputs": [],
      "source": [
        "!pip install dask[complete] h5py dask-ml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c7a9662",
      "metadata": {
        "id": "9c7a9662"
      },
      "source": [
        "## **1. Introducción**\n",
        "---\n",
        "\n",
        "La librería `dask_ml` nos permite entrenar modelos de machine learning e integrar librerías como `sklearn` para trabajar con grandes cantidades de datos sobre los arreglos y dataframes de `dask`.\n",
        "\n",
        "Primero, vamos a importar las librerías necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b18d2f13",
      "metadata": {
        "id": "b18d2f13"
      },
      "outputs": [],
      "source": [
        "import dask.array as da\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "735b317a",
      "metadata": {
        "id": "735b317a"
      },
      "source": [
        "## **2. Carga de Datos**\n",
        "---\n",
        "\n",
        "El proceso de carga de datos se puede realizar con la manipulación de arreglos o dataframes de `dask` como lo pudo ver en los notebooks anteriores.\n",
        "\n",
        "En este caso, usaremos el modulo `datasets` de `dask_ml` para cargar arreglos de `dask` directamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f126eee7",
      "metadata": {
        "id": "f126eee7"
      },
      "outputs": [],
      "source": [
        "from dask_ml.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples=1000, centers=3, random_state=42, chunks=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99b56751",
      "metadata": {
        "id": "99b56751"
      },
      "source": [
        "Como se puede observar, es equivalente a la función `make_blobs` de `sklearn, con la diferencia que acá se requiere especificar el parámetro `chunks` y que los datos creados son arreglos de `dask`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c32cb91",
      "metadata": {
        "id": "1c32cb91"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51efdae5",
      "metadata": {
        "id": "51efdae5"
      },
      "source": [
        "Ya que el conjunto de datos no es muy grande, podemos generar una visualización con `numpy` y `matplotlib`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cde93cb4",
      "metadata": {
        "id": "cde93cb4"
      },
      "outputs": [],
      "source": [
        "X_np, y_np = X.compute(), y.compute()\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X_np[:, 0], X_np[:, 1], c=y_np, alpha=0.1)\n",
        "ax.set_xlabel(\"$x_1$\")\n",
        "ax.set_ylabel(\"$x_2$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3f041b6",
      "metadata": {
        "id": "d3f041b6"
      },
      "source": [
        "## **3. Preprocesamiento**\n",
        "---\n",
        "\n",
        "`dask_ml` tiene un módulo de `preprocesamiento` bastante similar a `sklearn`, por ejemplo, podemos usar un `StandardScaler`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e761862",
      "metadata": {
        "id": "6e761862"
      },
      "outputs": [],
      "source": [
        "from dask_ml.preprocessing import StandardScaler\n",
        "norm = StandardScaler().fit(X)\n",
        "norm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b22d92f",
      "metadata": {
        "id": "7b22d92f"
      },
      "source": [
        "Podemos transformar los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a2e9b70",
      "metadata": {
        "id": "1a2e9b70"
      },
      "outputs": [],
      "source": [
        "X_t = norm.transform(X)\n",
        "X_t"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cfb22f2",
      "metadata": {
        "id": "0cfb22f2"
      },
      "source": [
        "## **4. Validación Cruzada**\n",
        "---\n",
        "\n",
        "El módulo `model_selection` de `dask_ml` es bastante similar al de `sklearn`, por ejemplo la función `train_test_split`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ea88afe",
      "metadata": {
        "id": "2ea88afe"
      },
      "outputs": [],
      "source": [
        "from dask_ml.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_t, y, test_size=0.3\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2785ce",
      "metadata": {
        "id": "da2785ce"
      },
      "source": [
        "Nuevamente, todos los resultados son arreglos de `dask`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65119f5e",
      "metadata": {
        "id": "65119f5e"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0718796f",
      "metadata": {
        "id": "0718796f"
      },
      "source": [
        "## **5. Modelamiento**\n",
        "---\n",
        "\n",
        "Tenemos distintas formas de usar modelos de forma distribuida en `dask_ml`, por un lado, los modelos internos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3060a05f",
      "metadata": {
        "id": "3060a05f"
      },
      "outputs": [],
      "source": [
        "from dask_ml.linear_model import LogisticRegression\n",
        "model = LogisticRegression().fit(X_train, y_train)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adfbe550",
      "metadata": {
        "id": "adfbe550"
      },
      "source": [
        "Como puede ver, el módulo `linear_model` es equivalente a `sklearn`, no obstante, podemos usar cualquier modelo de `sklearn` siempre que este tenga el método `partial_fit`, es decir, modelos que no requieren ver el conjunto de datos completo para su entrenamiento (únicamente algunas partes), en [este enlace](https://scikit-learn.org/0.15/modules/scaling_strategies.html) encuentra una descripción de qué modelos se pueden usar. Cualquiera de estos modelos se puede escalar con `dask_ml`, veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e6a4f1b",
      "metadata": {
        "id": "7e6a4f1b"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier # Modelo a escalar\n",
        "from dask_ml.wrappers import Incremental # Clase para escalar modelo\n",
        "model = Incremental(SGDClassifier(random_state=0, max_iter=100))\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10bc112a",
      "metadata": {
        "id": "10bc112a"
      },
      "source": [
        "En este caso, tenemos un nuevo modelo de `dask` que podemos usar como el modelo anterior, es decir, lo podemos entrenar con arreglos de `dask`, en este caso usamos `classes` para especificar los valores únicos que tienen las etiquetas (recuerde que el modelo no ve el conjunto de datos completo, entonces no lo puede saber de antemano)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef89cad7",
      "metadata": {
        "id": "ef89cad7"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train, classes=[0, 1, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c8e328",
      "metadata": {
        "id": "78c8e328"
      },
      "source": [
        "## **6. Evaluación**\n",
        "---\n",
        "\n",
        "`dask_ml` también tiene su paquete de métricas que permiten evaluarse sobre elementos de `dask` (algunas métricas no se pueden evaluar fácilmente, entonces es necesario hacerlo desde `sklearn`), primero, calculamos las predicciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "742fbf00",
      "metadata": {
        "id": "742fbf00"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a87d027",
      "metadata": {
        "id": "3a87d027"
      },
      "source": [
        "Ahora, evaluamos el accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cddaef09",
      "metadata": {
        "id": "cddaef09"
      },
      "outputs": [],
      "source": [
        "from dask_ml.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e34d6751",
      "metadata": {
        "id": "e34d6751"
      },
      "source": [
        "Como pudimos ver, `dask_ml` es una alternativa bastante interesante para poder entrenar modelos con grandes cantidades de datos, también resulta ser bastante útil por su alta compatibilidad con `sklearn`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd68718b",
      "metadata": {
        "id": "bd68718b"
      },
      "source": [
        "## **7. Recursos Adicionales**\n",
        "---\n",
        "\n",
        "* [DaskML](https://ml.dask.org/).\n",
        "* [Dask - Talks & tutorials](https://docs.dask.org/en/stable/presentations.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f0b9681",
      "metadata": {
        "id": "0f0b9681"
      },
      "source": [
        "## **8. Créditos**\n",
        "---\n",
        "\n",
        "**Profesor**\n",
        "\n",
        "- [Jorge E. Camargo, PhD](https://dis.unal.edu.co/~jecamargom/)\n",
        "\n",
        "**Diseño, desarrollo del notebook y material audiovisual**\n",
        "\n",
        "- [Juan S. Lara MSc](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/)\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}