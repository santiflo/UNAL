{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb9pUStUq6xf"
      },
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=1X3dvBPrNx6LbvsOJsYoZDOiUUwpYl-2v\" alt = \"Encabezado MLDS\" width = \"100%\">  </img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFWB01JlB2Mm"
      },
      "source": [
        "#**Reducción de la dimensionalidad**\n",
        "---\n",
        "\n",
        "En muchas tareas de análisis del mundo real el número de características a considerar es muy grande, dificultando el análisis por medio de visualización u otras tareas como el preprocesamiento o el modelado. La selección de las características es uno de los procesos de toma de decisiones más importantes en el aprendizaje automático, y en su desarrollo se han considerado técnicas de preprocesamiento que permiten reducir el número de dimensiones conservando la mayor cantidad de información posible. Este *notebook* se centra en esa necesidad y presenta la técnica de análisis de componentes principales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0sRGgMB2FUF"
      },
      "source": [
        "# **1. Dependencias**\n",
        "---\n",
        "Importamos las librerías necesarias y definimos algunas funciones básicas de visualización que vamos a usar en algunos ejemplos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLmUe5cmjYoJ"
      },
      "source": [
        "### **1.1. Dependencias**\n",
        "---\n",
        "Para la construcción de modelos y ejecución de procedimientos metodológicos de aprendizaje automático, utilizaremos la librería _Scikit-learn_ (**`sklearn`**) y varias de sus funciones y conjuntos de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A1bWvdAsW_B"
      },
      "source": [
        "# Actualizamos scikit-learn a la última versión\n",
        "!pip install -U scikit-learn\n",
        "\n",
        "# Importamos scikit-learn\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0jic_vwJv9c"
      },
      "source": [
        "Importamos además algunas librerías básicas y configuraciones de *Python*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5lk0elTiFy_"
      },
      "source": [
        "# Librerías básicas NumPy, Pandas, Matplotlib y Seaborn.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvYqmxWM8MbH"
      },
      "source": [
        "# Librería de visualización interactiva - Plotly\n",
        "!pip install -U plotly\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmx-RGqF8Hls"
      },
      "source": [
        "# Configuraciones para las librerías y módulos usados.\n",
        "\n",
        "# Ignoramos las advertencias o warnings.\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')\n",
        "\n",
        "# Configuramos el formato por defecto de la\n",
        "# librería de visualización Matplotlib.\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "mpl.rcParams['figure.dpi'] = 105\n",
        "mpl.rcParams['figure.figsize'] = (9, 7)\n",
        "sns.set_theme()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUMP_2CPqwRY"
      },
      "source": [
        "# Versiones de las librerías usadas.\n",
        "\n",
        "!python --version\n",
        "print('NumPy', np.__version__)\n",
        "print('Pandas', pd.__version__)\n",
        "print('Matplotlib', mpl.__version__)\n",
        "print('Seaborn', sns.__version__)\n",
        "print('Plotly', plotly.__version__)\n",
        "print('Scikit-learn', sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxNzX3sfq3gZ"
      },
      "source": [
        "Esta actividad se realizó con las siguientes versiones:\n",
        "*  *Python*: 3.7.10\n",
        "*  *NumPy*:  1.19.5\n",
        "*  *Pandas*: 1.1.5\n",
        "*  *Matplotlib*:  3.2.2\n",
        "*  *Seaborn*:  0.11.1\n",
        "*  *Plotly*: 4.14.3\n",
        "*  *Scikit-learn*: 0.24.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUdUDM9cjv1A"
      },
      "source": [
        "### **1.2. Funciones de utilidad y visualización**\n",
        "---\n",
        "\n",
        "Para ilustrar los ejemplos discutidos en este material utilizaremos algunas funciones que permiten visualizar de manera general los datos, junto a las funciones de predicción obtenidas con cada modelo.\n",
        "\n",
        "> **Nota**: *Matplotlib*, *Seaborn* y *Plotly* se encuentran por fuera del alcance de este módulo. No es necesario que entienda estas funciones en detalle para sacar partido del resto del contenido puesto a su disposición. Usted decide si leer o no estas funciones en profundidad. Si decide omitir esta sección, continúe directamente con la siguiente sección, en donde se discutirán los conjuntos de datos que vamos a utilizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru_SyCS8pDRw"
      },
      "source": [
        "# Función para visualizar un conjunto de datos en 2D\n",
        "\n",
        "def plot_data(X, y):\n",
        "    y_unique = np.unique(y)\n",
        "    colors = plt.cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))\n",
        "    for this_y, color in zip(y_unique, colors):\n",
        "        this_X = X[y == this_y]\n",
        "        plt.scatter(this_X[:, 0], this_X[:, 1],  color=color,\n",
        "                    alpha=0.5, edgecolor='k',\n",
        "                    label=\"Class %s\" % this_y)\n",
        "    plt.legend(loc=\"best\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYeSX9G75Fdf"
      },
      "source": [
        "# Grafica de la proyección en 2 dimensiones obtenida con PCA.\n",
        "\n",
        "def plot_pca(X, Xr, title = \"Proyección vectorial\", components = None):\n",
        "\n",
        "  plt.figure(dpi = 110, figsize = (7,6))\n",
        "  plt.title(title)\n",
        "  plt.scatter(X[:,0], X[:,1], color=\"blue\", alpha=.5, label=\"Datos originales\")\n",
        "  plt.scatter(Xr[:,0], Xr[:,1], color=\"red\", alpha=.5, label=\"Datos reconstruidos\")\n",
        "\n",
        "  plt.axis('equal')\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    plt.plot([X[i, 0], Xr[i, 0]], [X[i, 1], Xr[i, 1]], ls = '--', color = '#333333')\n",
        "\n",
        "  x_lo, x_hi = plt.xlim()\n",
        "  y_lo, y_hi = plt.ylim()\n",
        "\n",
        "  if not components is None:\n",
        "    for i in range(len(components)):\n",
        "      v = components[i]\n",
        "      plt.plot([-100* v[0], 0, 100* v[0]], [-100 * v[1], 0, 100 * v[1]], lw=2, ls = '--', label=f\"Componente principal {i + 1}\")\n",
        "\n",
        "  plt.ylim([-1.5, 1.5])\n",
        "  plt.xlim([-1.5, 1.5])\n",
        "  plt.legend(loc=\"center left\", bbox_to_anchor=(1.01,.5));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdjrpjFdBKkV"
      },
      "source": [
        "# Gráfica de la varianza explicada acumulada.\n",
        "\n",
        "def cumulative_explained_variance_plot(expl_variance):\n",
        "\n",
        "  cum_var_exp = np.cumsum(expl_variance)\n",
        "\n",
        "  plt.figure(dpi = 100, figsize = (8, 6))\n",
        "  plt.title('Curva acumulativa de la varianza explicada VS n° de componentes principales',\n",
        "            fontdict= dict(family ='serif', size = 16))\n",
        "  plt.xlabel('Número de componentes principales',\n",
        "             fontdict= dict(family ='serif', size = 14))\n",
        "  plt.ylabel('Varianza explicada acumulativa',\n",
        "             fontdict= dict(family ='serif', size = 14))\n",
        "\n",
        "  nc = np.arange(1, expl_variance.shape[0] + 1)\n",
        "\n",
        "  plt.plot(nc, cum_var_exp, '--r')\n",
        "  plt.plot(nc, cum_var_exp, 'c*', ms = 5)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cthA1L5GJxh"
      },
      "source": [
        "# Gráfica de las imágenes originales y reconstruidas.\n",
        "\n",
        "def show_img_matrix_pca(X, Xr):\n",
        "  plt.figure(figsize=(10,6), dpi = 105)\n",
        "  for i in range(6):\n",
        "    k = np.random.randint(len(X))\n",
        "\n",
        "    plt.subplot(3,6,i+1)\n",
        "    plt.imshow(Xr[k].reshape(28,28), cmap=plt.cm.Greys_r)\n",
        "    plt.xticks([]); plt.yticks([])\n",
        "\n",
        "    plt.subplot(3,6,6+i+1)\n",
        "    plt.imshow(X[k].reshape(28,28), cmap=plt.cm.Greys_r)\n",
        "    plt.xticks([]); plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RURwOVpHIb2P"
      },
      "source": [
        "# **2. Conjuntos de datos**\n",
        "---\n",
        "\n",
        "Para los ejemplos desarrollados en el transcurso de material, se usarán datos de  *Scikit-Learn* de carácter real (usando *Loaders* y otras fuentes) y sintético (usando *Generators*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_edMrC2xLG3e"
      },
      "source": [
        "### **2.1. Conjunto de datos *Wine***\n",
        "---\n",
        "\n",
        "El conjunto de datos *Wine* es uno de los más populares en el área del aprendizaje automático. Consiste en datos de un análisis químico de vinos de 3 cultivos diferentes de una región de Italia, y es comúnmente usado en tareas de clasificación. Hace parte del repositorio de *Machine Learning* de la Universidad de California en Irvine (UCI) y en este material se cargará con el método **`load_wine`** de **`sklearn.datasets`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyc4B8GlMJHB"
      },
      "source": [
        "# Conjuntos de datos - Wine\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine = load_wine(return_X_y = False)\n",
        "\n",
        "X_wine = wine.data\n",
        "y_wine = wine.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brZi_MfjMjhF"
      },
      "source": [
        "# Características del dataset\n",
        "wine.feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bteLwZh1MOHe"
      },
      "source": [
        "# Primeras 5 observaciones de las características de Wine\n",
        "pd.DataFrame(X_wine, columns = wine.feature_names).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBvjhT9TMUlD"
      },
      "source": [
        "# Primeras 5 observaciones de la variable objetivo.\n",
        "y_wine[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKgWHiHgMaE2"
      },
      "source": [
        "# Conteo de cada categoría de la variable objetivo\n",
        "pd.value_counts(y_wine)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v54n-Oq0KWrD"
      },
      "source": [
        "### **2.2. Digitos escritos a mano MNIST**\n",
        "---\n",
        "El *dataset* **MNIST** es un conjunto de datos de $60000$ imágenes de dígitos escritos a mano de $28 \\times 28$ píxeles. Es usada comúnmente en tareas de clasificación, donde la variable objetivo corresponde a un dígito de $0$ a $9$, y con $784$ características, que corresponden a cada píxel de la imagen. En esta ocasión, usaremos la función **`fetch_openml`** de **`sklearn.datasets`** para descargar el conjunto de datos del repositorio [OpenML.org](https://www.openml.org/d/554).\n",
        "\n",
        "> **Nota:** la carga del conjunto de datos puede tardar cerca de un minuto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66MLtcW3OEkv"
      },
      "source": [
        "#Conjuntos de datos - Repositorio OpenML\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Descargamos el dataset desde OpenML. Este proceso puede tardar un poco.\n",
        "mnist = fetch_openml('mnist_784', as_frame = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUqkxiLbuMeE"
      },
      "source": [
        "X_mnist = mnist.data\n",
        "y_mnist = mnist.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otm2z86AO1Na"
      },
      "source": [
        "# Primeras 5 observaciones.\n",
        "pd.DataFrame(X_mnist, columns = mnist.feature_names).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDwV_BlX9jLW"
      },
      "source": [
        "Cada número corresponde a un valor entre $0$ y $255$ con la intensidad del color, representado generalmente con escala de grises:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmi1KXNdPAiu"
      },
      "source": [
        "# Visualización de la primera imagen del dataset.\n",
        "plt.axis('off')\n",
        "plt.imshow(mnist.data[0].reshape(28,28), cmap = 'gray');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPrjUhTsPJrt"
      },
      "source": [
        "# Primeras 5 clases de la variable objetivo.\n",
        "y_mnist[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fjECLSlPbC-"
      },
      "source": [
        "# Conteo de la distribución de digitos\n",
        "pd.value_counts(y_mnist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZgbIJzaWLxG"
      },
      "source": [
        "#**3. Análisis de componentes principales (PCA)**\n",
        "---\n",
        "\n",
        "Inicialmente, consideraremos el caso en el que deseamos reducir un conjunto de datos de dos dimensiones a uno con solo una. Una alternativa es realizar una [proyección vectorial](https://matthew-brett.github.io/teaching/vector_projection.html) de nuestros datos en otro vector. La única dimensión de la nueva representación corresponderá a la magnitud del punto proyectado en la línea generada por el vector sobre el que se proyecta.\n",
        "Esta idea se puede ver mejor de forma gráfica. A continuación, creamos unos datos 2D de manera aleatoria.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2UNgaAwB2M9"
      },
      "source": [
        "# Creación de los datos sintéticos de ejemplo.\n",
        "\n",
        "# Semilla aleatoria.\n",
        "np.random.seed(1)\n",
        "\n",
        "# Datos sintéticos correlacionados.\n",
        "X = np.dot(np.random.random(size=(2, 2)),\n",
        "           np.random.normal(size=(2, 40))).T + 10\n",
        "\n",
        "# Centramos los datos en 0,0\n",
        "X = X - np.mean(X, axis=0)\n",
        "\n",
        "plt.figure(dpi = 110)\n",
        "plt.title('Datos sintéticos correlacionados', fontdict = dict(family = 'serif', size = 18))\n",
        "plt.scatter(X[:,0], X[:,1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w-EE6HOB2NK"
      },
      "source": [
        "En álgebra lineal, la proyección $proj_\\vec{v} \\vec{x}$ de un vector $\\vec{x}$ en otro vector $\\vec{v}$ se puede obtener mediante operaciones vectoriales:\n",
        "\n",
        "$$proj_\\vec{v} \\vec{x} = c \\vec{v}$$\n",
        "\n",
        "con $c$ igual a:\n",
        "\n",
        "$$c = \\frac{\\vec{v}\\cdot \\vec{x}}{||\\vec{v}||^2}$$\n",
        "\n",
        "\n",
        "$c$ es un escalar que refleja el punto de la proyección de $\\vec{x}$ sobre $\\vec{v}$. Este valor se puede interpretar como una representación de 1 dimensión sobre el vector $\\vec{v}$.\n",
        "\n",
        "Vamos a inspeccionar gráficamente algunas posibles proyecciones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpG8wuc3Hlxl",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#@markdown **Animación:** Ejecute esta celda para ver un ejemplo de proyección de un conjunto de datos de 2 dimensiones a un vector de 1 dimensión, con un registro de la varianza del vector resultante. Oprima el botón **'Reproducir'** para iniciar la animación.\n",
        "def unit_vector(angle):\n",
        "  return np.array([np.cos(angle), np.sin(angle)])\n",
        "\n",
        "n = 120\n",
        "angles = np.linspace(0, np.pi, n)\n",
        "projs = []\n",
        "\n",
        "frames_data = []\n",
        "for i in range(n):\n",
        "    angle = angles[i] # Ángulo entre 0 y pi.\n",
        "    v = unit_vector(angle) # Vector sobre el cual se realizará la proyección.\n",
        "    c = X.dot(v.reshape(-1,1))/(np.linalg.norm(v)**2) # Se calcula c con operación vectoriales de NumPy.\n",
        "    Xp = c * v # Valor en dos dimensiones con la posición de la proyección del vector.\n",
        "    frames_data.append( (Xp, angle, v, c))\n",
        "\n",
        "\n",
        "Xp0, angle0, v0, c0 = frames_data[0]\n",
        "\n",
        "fig = go.Figure(\n",
        "    data = [\n",
        "        go.Scatter(x = [-10* v0[0], 10* v0[0]], y = [-10 * v0[1], 10 * v0[1]], name = 'Vector de proyección', marker = dict(color = 'black'),\n",
        "                   text = angle0),\n",
        "        go.Scatter(x = [ -1.25], y = [ 0.75], text = f\"<b>Ángulo:</b> {angle0:.4f}\", textfont = dict(size = 20), textposition = 'top right',\n",
        "                   showlegend= False, mode = 'text'),\n",
        "        go.Scatter(x = [ -1.25], y = [ 0.65], text = f\"<b>Varianza de la proyección:</b> = {np.var(c0):.4f}\", textposition = 'top right',\n",
        "                   textfont = dict(size = 20), showlegend= False, mode = 'text'),\n",
        "            *[go.Scatter(x = [X[i, 0], Xp0[i, 0]], y = [X[i, 1], Xp0[i, 1]], line = dict(dash = 'dash'), showlegend= False,\n",
        "                         marker = dict(color = 'grey')) for i in range(len(X))],\n",
        "        go.Scatter(x = X[:,0], y= X[:,1], mode = 'markers', name = 'Datos originales', marker = dict(color = 'blue')),\n",
        "        go.Scatter(x = Xp0[:,0], y= Xp0[:,1], mode = 'markers', name = 'Datos reconstruidos', marker = dict(color = 'red'))\n",
        "        ],\n",
        "\n",
        "    layout= go.Layout(\n",
        "        height = 700,\n",
        "        width = 900,\n",
        "        xaxis=dict(range=[-2, 2], autorange=False),\n",
        "        yaxis=dict(range=[-1, 1], autorange=False),\n",
        "        title= dict(text = \"<b>Varianza de las posibles proyecciones vectoriales</b>\", font = dict(size = 25), xanchor = 'left'),\n",
        "        updatemenus=[dict(\n",
        "            type=\"buttons\",\n",
        "            buttons=[dict(label=\"Reproducir\",\n",
        "                          method=\"animate\",\n",
        "                          args=[None,  dict(frame = dict(duration = 50, redraw = False))]\n",
        "                          )])]\n",
        "    ),\n",
        "    frames  = [go.Frame(\n",
        "        data = [\n",
        "        go.Scatter(x = [-10* v[0], 10* v[0]], y = [-10 * v[1], 10 * v[1]], name = 'Vector de proyección', marker = dict(color = 'black')),\n",
        "        go.Scatter(x = [ -1.25], y = [ 0.75], text = f\"<b>Ángulo:</b> {angle:.4f}\", textfont = dict(size = 20),\n",
        "                   textposition = 'top right',\n",
        "                   showlegend= False, mode = 'text'),\n",
        "        go.Scatter(x = [ -1.25], y = [ 0.65], text = f\"<b>Varianza de la proyección:</b> = {np.var(c):.4f}\", textposition = 'top right',\n",
        "                   textfont = dict(size = 20), showlegend= False, mode = 'text'),\n",
        "                *[go.Scatter(x = [X[i, 0], Xp[i, 0]], y = [X[i, 1], Xp[i, 1]], line = dict(dash = 'dash'), showlegend= False,\n",
        "                             marker = dict(color = 'grey')) for i in range(len(X))],\n",
        "        go.Scatter(x = X[:,0], y= X[:,1], mode = 'markers', name = 'Datos originales', marker = dict(color = 'blue')),\n",
        "        go.Scatter(x = Xp[:,0], y= Xp[:,1], mode = 'markers', name = 'Datos reconstruidos', marker = dict(color = 'red'))\n",
        "                ]) for Xp, angle, v, c in frames_data]\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Jj6dIVB2NW"
      },
      "source": [
        "Ahora note que las proyecciones que permiten distinguir más entre observaciones de los datos originales y que más se ajustan a ellos son las que tienen mayor **variabilidad o varianza**. Por lo general, queremos encontrar una proyección con menos dimensiones que preserve la máxima cantidad de variabilidad. Este es la idea detrás de la técnica **PCA**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzkZ2TRFdLwN"
      },
      "source": [
        "## **3.1. Análisis de componentes principales**\n",
        "---\n",
        "El **análisis de componentes principales** (llamado comúnmente por sus siglas en inglés [PCA](https://es.wikipedia.org/wiki/An%C3%A1lisis_de_componentes_principales)) es una técnica utilizada para describir un conjunto de datos en términos de nuevas variables (componentes) no correlacionadas. Las componentes son ordenadas por la cantidad de varianza original que son capaces de describir y es útil para reducir la dimensionalidad de un conjunto de datos.\n",
        "\n",
        "La técnica *PCA* busca la proyección según la cual los datos queden mejor representados en términos de mínimos cuadrados, y convierte un conjunto de observaciones de variables posiblemente correlacionadas en un conjunto de valores de variables sin correlación lineal llamadas componentes principales. Esta técnica se emplea principalmente en el análisis exploratorio de datos y en la construcción de modelos predictivos.\n",
        "\n",
        "**Limitaciones:**\n",
        "  * Se asume que los datos observados son combinación lineal de una cierta base.\n",
        "  * *PCA* utiliza los vectores propios de la matriz de covarianzas y sólo encuentra las direcciones de ejes en el espacio de variables considerando que los datos tienen una distribución normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZGXUmdPsPrH"
      },
      "source": [
        "<center><img src = \"https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2018/07/pca.gif\" alt = \"PCA\" width = \"70%\">  </img></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT4Xisxo1S2h"
      },
      "source": [
        "*Scikit-Learn* permite aplicar un algoritmo de *PCA* sobre un conjunto de datos mediante la función **`sklearn.decomposition.PCA`**. Esta recibe el argumento **`n_components`** que define la dimensión de la nueva representación.\n",
        "\n",
        "Al igual que con otros estimadores y modelos de **`sklearn`**, este objeto cuenta con la función **`.fit()`** para ingresar los datos que se desean descomponer con PCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2kJ4qXS1TVG"
      },
      "source": [
        "# Análisis de componentes principales (PCA)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=1)\n",
        "pca.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUzczo3FB2Ob"
      },
      "source": [
        "Para nuestro ejemplo, podemos usar el componente mayor para reducir la dimensionalidad de nuestros datos de $2$ dimensiones a solamente $1$.\n",
        "\n",
        "Observe que:\n",
        "\n",
        "$$\\mathbf{X_t} = \\mathbf{X} \\cdot \\mathbf{V_c}$$\n",
        "\n",
        "donde:\n",
        "- $\\mathbf{X}$ son nuestros datos\n",
        "- $\\mathbf{V_c}$ es el vector de componentes seleccionados\n",
        "- $\\mathbf{X_t}$ son los datos transformados\n",
        "\n",
        "En este caso nos estamos restringiendo a **transformaciones lineales**, es decir, rotaciones y escalado. Para realizar este proceso con *Scikit-Learn* utilizamos el método **`.transform()`**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU3X89P1B2Od"
      },
      "source": [
        "# Transformación de los datos originales en la proyección de menor dimensión.\n",
        "Xt = pca.transform(X)\n",
        "\n",
        "# Datos con una dimensión (característica) menos.\n",
        "print(X.shape, Xt.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIdKf7Ec3PyC"
      },
      "source": [
        "Es muy común que los datos usados en la obtención de los componentes (usando del método **`fit`**) sean los mismos que queramos transformar a una dimensión menor. Este proceso se puede realizar en un solo paso utilizando el método **`fit_transform`** de **`PCA`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16dLmN9j3lIN"
      },
      "source": [
        "pca = PCA(n_components= 1)\n",
        "Xft = pca.fit_transform(X)\n",
        "\n",
        "# Datos con una dimensión menos. Igual que usar fit y después transform.\n",
        "Xft.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIvSkVNUB2Oo"
      },
      "source": [
        "plt.figure(dpi = 105, figsize = (10,4))\n",
        "\n",
        "# Datos originales\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(X[:,0], X[:,1], color=\"blue\", alpha=.5, label=\"$\\mathbf{X}$: Datos originales\");\n",
        "plt.axis(\"equal\"); plt.legend();\n",
        "\n",
        "# Datos reducidos\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(Xft, np.zeros(len(Xft)), color=\"red\", alpha=.5, label=\"$\\mathbf{X_t}$: Datos reducidos\");\n",
        "plt.axis(\"equal\"); plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZZEjtqoB2Om"
      },
      "source": [
        "También podemos reconstruir los datos a la dimensión original a partir de datos obtenidos mediante una transformación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENidihO5gA_r"
      },
      "source": [
        "Xr = pca.inverse_transform(Xt)\n",
        "\n",
        "plot_pca(X, Xr,\n",
        "         title = \"Reconstrucción a partir del componente mayor\",\n",
        "         components = pca.components_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1dRBUHjF2bR"
      },
      "source": [
        "Con el atributo **`components_`** podemos acceder a los vectores de los componentes principales obtenidos con PCA. Obtengamos $2$ componentes principales y veamos su representación gráfica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJIZJpyycTD-"
      },
      "source": [
        "pca = PCA(n_components= 2)\n",
        "Xp = pca.fit_transform(X)\n",
        "Xr = pca.inverse_transform(Xp)\n",
        "\n",
        "pca.components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROEyBr9Aakfr"
      },
      "source": [
        "# Representamos con vectores los componentes principales.\n",
        "plot_pca(X,\n",
        "         Xr,\n",
        "         title = \"Componentes principales\",\n",
        "         components = pca.components_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hclATfELjpji"
      },
      "source": [
        "##**3.2. Varianza explicada para las componentes principales**\n",
        "---\n",
        "\n",
        "En *PCA*, la varianza total es la suma de las varianzas de todos los componentes principales individuales.\n",
        "Entonces, la **proporción de varianza explicada** por un componente principal es la relación entre la varianza de ese componente principal y la varianza total.\n",
        "Por otro lado, si se quiere conocer el porcentaje de varianza total de varios componentes principales se puede obtener con la suma de sus varianzas y entre la varianza total.\n",
        "\n",
        "Esto se hace generalmente con los componentes ordenados por mayor varianza explicada y se representa gráficamente con una gráfica de líneas con la **varianza explicada acumulada**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlPx8HSg-0E1"
      },
      "source": [
        "pca = PCA(n_components = 2)\n",
        "Xt = pca.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts_r_B8veC8K"
      },
      "source": [
        "La varianza explicada se obtiene del atributo  **`explained_variance_ratio_`**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuFt3KnpFFXu"
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGVrJ2UVenph"
      },
      "source": [
        "El resultado indica que el primer componente explica el $97.6\\%$ de la varianza y el segundo componente el $2.4\\%$ restante.\n",
        "\n",
        "En este caso, realizar la gráfica de la varianza acumulada no tiene mucho sentido pues solo contamos con dos columnas. Veamos la aplicación de PCA en un contexto real. Primero, cargamos el conjunto de datos ***Wine***:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPBR8WBGzVgt"
      },
      "source": [
        "wine = load_wine()\n",
        "\n",
        "labels = wine.target\n",
        "classes = wine.target_names\n",
        "\n",
        "wine_df = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
        "wine_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw9AFfxahVnz"
      },
      "source": [
        "Ahora, realizamos un escalado de los datos, con media en 0 y varianza en 1. Esto se puede realizar con el método **`sklearn.preprocessing.StandardScaler`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT98nL476wad"
      },
      "source": [
        "# Preprocesamiento (Reescalado estándar)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc_x = StandardScaler()\n",
        "sc_x.fit(wine_df.values)\n",
        "X_scaled = sc_x.transform(wine_df.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqpFhhBphqon"
      },
      "source": [
        "X_scaled.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMZgz0M95lB5"
      },
      "source": [
        "Analizando la varianza explicada de las componentes principales obtenemos lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgcMYW2l5q2p"
      },
      "source": [
        "# Si no se indica el número de componentes se usa la cantidad de columnas.\n",
        "pca = PCA()\n",
        "transf = pca.fit_transform(X_scaled)\n",
        "\n",
        "varianza_expl = pca.explained_variance_ratio_\n",
        "\n",
        "print(varianza_expl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RQR3VVssnrH"
      },
      "source": [
        "A continuación, graficamos la curva acumulativa de la varianza explicada versus el número de componentes principales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CGJgr22Ba2H"
      },
      "source": [
        "cumulative_explained_variance_plot(varianza_expl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeHd6T6mjFRG"
      },
      "source": [
        "Podemos observar lo siguiente:\n",
        "  * Las componentes principales $1$ y $2$ representan el $55,4\\%$ de la varianza de los datos\n",
        "  * Las $7$ primeras componentes principales representan el $89,3\\%$ de la varianza de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnOG1Rmtt3hX"
      },
      "source": [
        "Podemos usar los componentes principales $1$ y $2$ para visualizar el conjunto de datos en $2$ dimensiones. Para hacer esto usamos el comando **`sklearn_pca.fit_transform`**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-HVcwwatjC7"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_transf = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize = (10, 8), dpi = 105)\n",
        "plt.xlabel('Componente principal 1')\n",
        "plt.ylabel('Componente principal 2')\n",
        "plt.title('Vectores singulares más significativos después de la transformación lineal a través de PCA')\n",
        "\n",
        "plot_data(X_transf, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y064QOwekNUO"
      },
      "source": [
        "A pesar de ser un *dataset* de $13$ características, somos capaces de distinguir sus valores de forma gráfica en $2$ dimensiones usando sus componentes principales. Al ser los dos componentes más importantes, su visualización es la que contiene mayor varianza y es más fácil de distinguir entre sus observaciones. Ahora, consideremos una gráfica con los componentes principales $2$ y $3$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y1Npbsrtm3_"
      },
      "source": [
        "pca = PCA(n_components=3)\n",
        "X_transf = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize = (10, 8), dpi = 105)\n",
        "plt.xlabel('Componente principal 2')\n",
        "plt.ylabel('Componente principal 3')\n",
        "plt.title('Vectores singulares más significativos después de la transformación lineal a través de PCA')\n",
        "plot_data(X_transf[:,1:3],labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvVynin1Zk7j"
      },
      "source": [
        "# **4. Aplicación en clasificación**\n",
        "---\n",
        "Ahora, veremos la aplicación de este método de reducción de la dimensionalidad en tareas de modelado como clasificación y agrupamiento, usando como datos de entrenamiento la representación obtenida con PCA.\n",
        "> **¿Qué tan conveniente es este acercamiento al problema?**\n",
        "\n",
        "En esta parte trabajaremos con una muestra del conjunto de imágenes de **MNIST** que contiene únicamente $1500$ imágenes de dígitos escritos a mano. Aplicaremos reducción de dimensionalidad y usaremos los datos reducidos con esta herramienta en tareas de clasificación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWKi5hxMaDKv"
      },
      "source": [
        "X_mnist = mnist.data[:1500]\n",
        "y_mnist = mnist.target[:1500]\n",
        "print(\"Dimensión de las imágenes y las clases: \", X_mnist.shape, y_mnist.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daaeu7pnm-E0"
      },
      "source": [
        "Visualicemos una muestra aleatoria de imágenes del *dataset* con el método de visualización de imágenes de *Matplotlib* **`imshow`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkG-VkdbaDuR"
      },
      "source": [
        "perm = np.random.permutation(range(X_mnist.shape[0]))[0:50]\n",
        "\n",
        "random_imgs   = X_mnist[perm]\n",
        "random_labels = y_mnist[perm]\n",
        "\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "for i in range(random_imgs.shape[0]):\n",
        "    ax=fig.add_subplot(5,10,i+1)\n",
        "    plt.imshow(random_imgs[i].reshape(28,28), interpolation=\"nearest\", cmap = plt.cm.Greys_r)\n",
        "    ax.set_title(int(random_labels[i]))\n",
        "    plt.xticks([]); plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCsQHBsr7ub4"
      },
      "source": [
        "A continuación, graficamos la curva acumulativa de la varianza explicada versus el número de componentes principales:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uldj_6rI7umH"
      },
      "source": [
        "# PCA en MNIST\n",
        "pca = PCA(n_components=None)\n",
        "X_mnist_transf = pca.fit_transform(X_mnist)\n",
        "varianza_expl = pca.explained_variance_ratio_\n",
        "\n",
        "cum_var_exp = np.cumsum(varianza_expl)\n",
        "\n",
        "cumulative_explained_variance_plot(varianza_expl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZxURmSwFYIB"
      },
      "source": [
        "print(f'Primeras 10 componentes: {cum_var_exp[9]}')\n",
        "print(f'Primeras 60 componentes: {cum_var_exp[59]}')\n",
        "print(f'Primeras 100 componentes: {cum_var_exp[99]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_KhKccjFzwH"
      },
      "source": [
        "Podemos observar lo siguiente:\n",
        "  * Las $10$ primeras componentes principales representan el $49,2\\%$ de la varianza de los datos\n",
        "  * Las $60$ primeras componentes principales representan el $86,2\\%$ de la varianza de los datos\n",
        "  * Las $100$ primeras componentes principales representan el $92,3\\%$ de la varianza de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Aqu5HSlAsnA"
      },
      "source": [
        "A continuación, usamos las primeras $10$ componentes principales y graficaremos la transformación inversa:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCrM6MF1_kdc"
      },
      "source": [
        "pca = PCA(n_components=10)\n",
        "Xp = pca.fit_transform(X_mnist)\n",
        "Xr = pca.inverse_transform(Xp)\n",
        "\n",
        "show_img_matrix_pca(X_mnist, Xr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OIWLGabA0QT"
      },
      "source": [
        "Los datos reconstruidos se asemejan bastante a los datos originales. Un proceso similar es el realizado en la [compresión de imágenes](https://en.wikipedia.org/wiki/Image_compression), en el cual se conserva la información más relevante y  necesaria para que sea reconocible.\n",
        "Ahora usemos las primeras $60$ componentes principales:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awrbkh7oaPUl"
      },
      "source": [
        "pca = PCA(n_components=60)\n",
        "Xp = pca.fit_transform(X_mnist)\n",
        "Xr = pca.inverse_transform(Xp)\n",
        "\n",
        "show_img_matrix_pca(X_mnist, Xr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh-bH2uKBSdZ"
      },
      "source": [
        "Y con las primeras $100$ componentes principales:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK1sQUrHBVKl"
      },
      "source": [
        "pca = PCA(n_components=100)\n",
        "Xp = pca.fit_transform(X_mnist)\n",
        "Xr = pca.inverse_transform(Xp)\n",
        "\n",
        "show_img_matrix_pca(X_mnist, Xr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH7ltuV_HpDu"
      },
      "source": [
        "\n",
        "Los datos resultantes no están organizados como en los pixeles de una imágen, sino que corresponden a la codificación numérica de la información más importante con menos dimensiones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqmMPlTQHp3w"
      },
      "source": [
        "print(Xr[0,:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4XMb-uSaSlz"
      },
      "source": [
        "Finalmente, realicemos el proceso de modelado de clasificación de las imágenes, y comparemos los resultados entre las dos opciones. En este caso usaremos un clasificador bayesiano con el método **`sklearn.naive_bayes.GaussianNB`**.\n",
        "\n",
        "Los clasificadores *Naive Bayes* están basados en métodos de clasificación bayesiana y en el teorema de *Bayes*, que describe la relación de las probabilidades condicionales de cantidades estadísticas. Este método funciona de forma similar a los demás clasificadores vistos en el curso.\n",
        "\n",
        "Veamos la efectividad de este modelo tanto para los datos originales como para la nueva representación obtenida con _PCA_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGVpFQndaS7K"
      },
      "source": [
        "# Selección de modelos y validación cruzada.\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Métodos de modelado - Clasificación con NaiveBayes Gaussiano.\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "print(f\"Accuracy con datos originales: {np.mean(cross_val_score(GaussianNB(), X_mnist, y_mnist, cv=5)):.4f}\")\n",
        "print(f\"Accuracy con la nueva representación: {np.mean(cross_val_score(GaussianNB(), Xp, y_mnist, cv=5)):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJWtH_i1aXAg"
      },
      "source": [
        "Podemos notar que al usar los datos de entrenamiento reducidos obtenemos una mayor precisión o *accuracy* promedio que al usar todos los datos originales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r5hSQAz6d-g"
      },
      "source": [
        "# **Recursos adicionales**\n",
        "---\n",
        "Los siguientes enlaces corresponden a sitios en donde encontrará información muy útil para profundizar en el conocimiento de las funcionalidades de la librería *Scikit-learn* en la aplicación de técnicas de reducción de la dimensionalidad y análisis de componentes principales, además de material de apoyo teórico para reforzar estos conceptos:\n",
        "\n",
        "* **Introductorios**\n",
        "  * [A One-Stop Shop for Principal Component Analysis - Matt Brems](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)\n",
        "  * [PCA example with Iris Data-set](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py)\n",
        "  * [Manifold Hypothesis - Deep AI glossary](https://deepai.org/machine-learning-glossary-and-terms/manifold-hypothesis)\n",
        "  * [Dimensionality Reduction, PCA Intro - AI Pool](https://ai-pool.com/a/s/dimensionality-reduction--pca-intro)\n",
        "  * [Principal component analysis explained simply - BioTuring's Blog](https://blog.bioturing.com/2018/06/14/principal-component-analysis-explained-simply/)\n",
        "\n",
        "* **Avanzados**\n",
        "  * [Mathematics For Machine Learning Chapter 10 - Dimensionality Reduction with Principal Component Analysis](https://mml-book.github.io/book/mml-book.pdf)\n",
        "\n",
        "  * [John Shlens - A Tutorial on Principal Component Analysis](https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6dwABE_Zg44"
      },
      "source": [
        "# **Créditos**\n",
        "---\n",
        "\n",
        "* **Profesor:** [Fabio Augusto Gonzalez](https://dis.unal.edu.co/~fgonza/)\n",
        "* **Asistentes docentes:**\n",
        "  * Miguel Angel Ortiz Marín\n",
        "  * Alberto Nicolai Romero Martínez\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ]
}